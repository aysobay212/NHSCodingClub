from random import seed
from random import randrange
from csv import reader
from math import exp
from csv import reader
import numpy as np
import matplotlib.pyplot as plt
from numpy.lib.arraysetops import unique


def load_csv(filename, skip = False):
    dataset = list()     
    with open(filename, 'r') as file:
        csv_reader = reader(file)
        
        # Skip the header, if needed
        if skip:
            next(csv_reader, None)
        
        for row in csv_reader:
            dataset.append(row)
        
    return dataset




# Assign the diagnosis of mailgnant (M) to 0 and assign the diagnosis of benign (B) to 1
def diagnosis_column_to_number(dataset, column):
    for row in dataset:
        if row[column] == 'M':
            row[column] = 0
        elif row[column] == 'B':
            row[column] = 1


def extract_only_x_data(dataset):
    data = list()
    for i in range(0, len(dataset)):
        data.append(list())
        
        for j in range(0, len(dataset[i]) - 1):
            data[-1].append(float(dataset[i][j]))
    return data

def extract_only_y_data(dataset):
    data = list()
    for i in range(0, len(dataset)):
        data.append(float(dataset[i][-1]))
    return data

def sigmoid(z):
    z = 1/(1 + np.exp(-z))
    return z



def loss(y, y_hat):
    #note that y_hat has passed the sigmoid, this is the h_theta(x) - AYDEN
    loss = -np.mean(y * (np.log(y_hat)) - (1-y) * np.log(1-y_hat))


    return loss


def gradients(X, y, y_hat):
    
    # X Input.
    # y true/target value.
    # y_hat predictions.
    # w weights.
    # b bias.
    
    # number of training examples.
    numner_of_examples = X.shape[0]
    
    # Gradient of loss weights.
    dw = (1/numner_of_examples)*np.dot(X.T, (y_hat - y))
    
    # Gradient of loss bias.
    db = (1/numner_of_examples)*np.sum((y_hat - y)) 
    
    return dw, db

def train(X, y, batch_size, epochs, learning_rate):
    
    # X Input.
    # y true/target value.
    # batch_size Batch Size.
    # epochs Number of iterations.
    # learning_rate Learning rate.
        
    # number of training examples
    # number of features 
    numner_of_examples, numner_of_features = X.shape
    
    # Initializing weights and bias to zeros.
    weights = np.zeros((numner_of_features,1))
    bias = 0
    
    # Reshaping y.
    y = y.reshape(numner_of_examples,1)
    
    # Empty list to store losses.
    losses = []
    
    # Training loop.
    for epoch in range(epochs):
        for i in range((numner_of_examples-1)//batch_size + 1):
            
            # Defining batches. SGD.
            start_i = i * batch_size
            end_i = start_i + batch_size
            xb = X[start_i:end_i]
            yb = y[start_i:end_i]
            
            # Calculating hypothesis/prediction.
            y_hat = sigmoid(np.dot(xb, weights) + bias)
            
            # Getting the gradients of loss w.r.t parameters.
            dw, db = gradients(xb, yb, y_hat)
            
            # Updating the parameters.
            weights = weights - (learning_rate * dw)
            bias = bias - (learning_rate * db)     
        
        # Calculating loss and appending it in the list.
        l = loss(y, sigmoid(np.dot(X, weights) + bias))
        losses.append(l)
        
    # returning weights, bias and losses(List).
    return weights, bias, losses
    
def predict(X, w, b):
    
    # X Input
    # Calculating presictions/y_hat.
    preds = sigmoid(np.dot(X, w) + b)
    
    # Empty List to store predictions.
    pred_class = []
        
    # if y_hat >= 0.5 round up to 1
    # if y_hat < 0.5 round down to 0
    for i in preds:
        if i >= 0.5:
            pred_class.append(1)
        else:
            pred_class.append(0)

    return np.array(pred_class)

def accuracy(y, y_hat):
    accuracy = np.mean(y == y_hat)
    return accuracy

def plot_decision_boundary(X, w, b):
    
    # X Inputs
    # w weights
    # b bias
    
    fig = plt.figure(figsize=(10,8))
    plt.plot(X[:, 0][y==0], X[:, 1][y==0], "g^")
    plt.plot(X[:, 0][y==1], X[:, 1][y==1], "bs")
    plt.xlim([-2, 2])
    plt.ylim([0, 2.2])
    plt.xlabel("feature 1")
    plt.ylabel("feature 2")
    plt.title('Decision Boundary')
    
    # The Line is y=mx+c
    # So, Equate mx+c = w.X + b
    # Solving we find m and c
    x1 = [min(X[:,0]), max(X[:,0])]
    
    if(w[1] != 0):
        m = -w[0]/w[1]
        c = -b/w[1]
        x2 = m*x1 + c
        plt.plot(x1, x2, 'y-')
    
    plt.show()

filename = 'breast_cancer_data.csv'
dataset = load_csv(filename, skip=True)
diagnosis_column_to_number(dataset, 2)

X_train_data = extract_only_x_data(dataset) #returns list of list
y_train_data = extract_only_y_data(dataset) #returns list
X = np.array(X_train_data) 
y = np.array(y_train_data)

# Training 
w, b, l = train(X, y, batch_size=100, epochs=1000, learning_rate=0.01)
# Plotting Decision Boundary
plot_decision_boundary(X, w, b)

print(accuracy(y, y_hat=predict(X, w, b)))
